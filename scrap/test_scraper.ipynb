{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctg-studies_20250301_232318.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "archive_dir = 'raw_data_archive'\n",
    "files = [f for f in os.listdir(archive_dir) if f.startswith(\"ctg-studies_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not files:\n",
    "    print(\"No archived files found.\")\n",
    "\n",
    "# Extract timestamps and sort by most recent\n",
    "files.sort(key=lambda f: time.strptime(re.search(r'_(\\d{8}_\\d{6})', f).group(1), \"%Y%m%d_%H%M%S\"), reverse=True)\n",
    "latest_file = files[0]\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_ctg():\n",
    "    # ðŸ“Œ Set download directory to current folder\n",
    "    download_dir = os.getcwd()  \n",
    "\n",
    "    # **Set Chrome options for automatic downloads**\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")  # Maximize browser\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Reduce bot detection\n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_dir,  # Save downloads in current directory\n",
    "        \"download.prompt_for_download\": False,       # No prompt\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True                 # Allow safe downloads\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # **Initialize WebDriver**\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # **Open the website**\n",
    "    driver.get(\"https://clinicaltrials.gov/search\")\n",
    "\n",
    "    try:\n",
    "        # ðŸ•’ Wait for button to open modal\n",
    "        main_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"action-bar-button\"))\n",
    "        )\n",
    "        main_button.click()\n",
    "        \n",
    "        # ðŸ•’ Wait for download button inside the modal\n",
    "        download_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'usa-button') and contains(@class, 'primary-button')]\"))\n",
    "        )\n",
    "\n",
    "        # âœ… Scroll into view before clicking\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", download_button)\n",
    "        time.sleep(1)  # Allow scrolling time\n",
    "\n",
    "        # âœ… Click the download button\n",
    "        download_button.click()\n",
    "        print(\"âœ… Download started!\")\n",
    "\n",
    "        # ðŸ•’ Wait for the file to be downloaded\n",
    "        timeout = 180  # Max wait time (seconds)\n",
    "        start_time = time.time()\n",
    "\n",
    "        while time.time() - start_time < timeout:\n",
    "            # List files in the download directory\n",
    "            files = os.listdir(download_dir)\n",
    "            \n",
    "            # Check if the `.crdownload` file still exists\n",
    "            crdownload_files = [f for f in files if f.endswith(\".crdownload\")]\n",
    "            if not crdownload_files:\n",
    "                # If no `.crdownload` files exist, assume download is complete\n",
    "                completed_file = [f for f in files if f.endswith(\".csv\")][0] if [f for f in files if f.endswith(\".csv\")] else None\n",
    "                if completed_file:\n",
    "                    print(f\"âœ… File downloaded: {completed_file}\")\n",
    "                    break\n",
    "            \n",
    "            # Wait and check again\n",
    "            time.sleep(1)\n",
    "\n",
    "        if crdownload_files:\n",
    "            print(\"âŒ Download did not complete within the expected time frame.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error:\", e)\n",
    "\n",
    "    # Print title to confirm success\n",
    "    print(\"Page Title:\", driver.title)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "def scrape_eudraCT_page(page, writer):\n",
    "    URL = f\"https://www.clinicaltrialsregister.eu/ctr-search/search?query=&page={page}\"\n",
    "\n",
    "    # Send a GET request\n",
    "    response = requests.get(URL)\n",
    "\n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    results_div = soup.find(\"div\", class_=\"results\")\n",
    "    tables = results_div.find_all(\"table\", class_=\"result\")\n",
    "    # Loop through each table\n",
    "    # with open(\"clinical_trials.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    #     writer = csv.writer(file)\n",
    "            \n",
    "    # Write the header row\n",
    "    writer.writerow([\"Study Identifier\", \"Sponsor Name\", \"Study Name\", \"Medical Condition\"])\n",
    "\n",
    "    # Loop through each table\n",
    "    for table_index, table in enumerate(tables, start=1):\n",
    "\n",
    "        # Find all rows (tr) in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Initialize variables to store extracted data\n",
    "        study_identifier = sponsor_name = study_name = condition = \"N/A\"\n",
    "\n",
    "        for row_index, row in enumerate(rows, start=1):\n",
    "            # Get all table cells (td) within each row\n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.text.strip() for cell in cells]  # Extract text from each cell\n",
    "                    \n",
    "            # Extract relevant data from each row\n",
    "            if row_index == 1 and row_data:\n",
    "                study_identifier = row_data[0].split(\":\")[1].strip()\n",
    "            if row_index == 2 and row_data:\n",
    "                sponsor_name = row_data[0].split(\":\")[1].strip() \n",
    "            if row_index == 3 and row_data:\n",
    "                study_name = row_data[0].split(\":\")[1].strip() \n",
    "            if row_index == 4 and row_data:\n",
    "                condition = row_data[0].split(\":\")[1].strip()\n",
    "\n",
    "        # Write extracted data into the CSV file\n",
    "        writer.writerow([study_identifier, sponsor_name, study_name, condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clinical_trials.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for page in range(1,4):\n",
    "        scrape_eudraCT_page(page, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_eudraCT(file_name, start_page, end_page):\n",
    "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for page in range(start_page, end_page+1):\n",
    "            scrape_eudraCT_page(page, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_eudraCT(\"clinical_trials.csv\",start_page=1, end_page=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing data from us\n",
      "Inserted 62 records into us\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Set up the database connection\n",
    "conn = psycopg2.connect(\n",
    "    dbname='miracle_scrap',\n",
    "    user='edwardch',\n",
    "    password='123456',\n",
    "    host='localhost',\n",
    "    port='5434'  # Default PostgreSQL port\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "table_name = 'us'\n",
    "df = pd.read_csv(\"clinical_trials.csv\")\n",
    "# Overwrite: Delete existing data before inserting new data\n",
    "cursor.execute(f\"TRUNCATE TABLE raw.{table_name} RESTART IDENTITY;\")\n",
    "conn.commit()\n",
    "print(f\"Cleared existing data from {table_name}\")\n",
    "\n",
    "# Insert new data\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO raw.{table_name} (study_identifier, study_title, conditions, sponsor)\n",
    "        VALUES (%s, %s, %s, %s);\n",
    "    \"\"\", (row['Study Identifier'], row['Study Name'], row['Medical Condition'], row['Sponsor Name']))\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(df)} records into {table_name}\")\n",
    "\n",
    "# Close connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_csv(file_name, table_name):\n",
    "    try:\n",
    "        # Read CSV into a DataFrame\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        # renaming EudraCT columns to match the format of clinicaltrials.gov\n",
    "        if table_name == 'eu':\n",
    "            df.columns = ['study_identifier', 'study_title', 'study_url', 'status', 'conditions', 'interventions', 'sponsor', 'collaborators', 'study_type']\n",
    "\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(\n",
    "            dbname='miracle_scrap',\n",
    "            user='edwardch',\n",
    "            password='123456',\n",
    "            host='localhost',\n",
    "            port='5434'  # Default PostgreSQL port\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Overwrite: Delete existing data before inserting new data\n",
    "        cursor.execute(f\"TRUNCATE TABLE raw.{table_name} RESTART IDENTITY;\")\n",
    "        conn.commit()\n",
    "        print(f\"Cleared existing data from {table_name}\")\n",
    "\n",
    "        # Insert new data\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO raw.{table_name} (study_identifier, study_title, conditions, sponsor)\n",
    "                VALUES (%s, %s, %s, %s);\n",
    "            \"\"\", (row['study_identifier'], row['study_title'], row['conditions'], row['sponsor']))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Inserted {len(df)} records into {table_name}\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edwardch/Desktop/miracle_scraper/download\n"
     ]
    }
   ],
   "source": [
    "curr = os.getcwd()\n",
    "download = os.path.join(curr,'download')\n",
    "print(download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miracle_scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
